{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get required libraries \n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train and test datasets and shuffle the data \n",
    "import time\n",
    "start = time.clock()\n",
    "train = shuffle(pd.read_csv(\"train.csv\"))\n",
    "test = shuffle(pd.read_csv(\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Train set (7352, 561)\n",
      "Dimension of Test set (2947, 561) \n",
      "\n",
      "Number of numeric features: 561\n"
     ]
    }
   ],
   "source": [
    "# Seperating Predictors and Outcome values from train and test sets\n",
    "X_train = pd.DataFrame(train.drop(['Activity','subject'],axis=1))\n",
    "Y_train_label = train.Activity.values.astype(object)\n",
    "X_test = pd.DataFrame(test.drop(['Activity','subject'],axis=1))\n",
    "Y_test_label = test.Activity.values.astype(object)\n",
    "\n",
    "# Dimension of Train and Test set \n",
    "print(\"Dimension of Train set\",X_train.shape)\n",
    "print(\"Dimension of Test set\",X_test.shape,\"\\n\")\n",
    "\n",
    "# Transforming non numerical labels into numerical labels\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encoding train labels \n",
    "encoder.fit(Y_train_label)\n",
    "Y_train = encoder.transform(Y_train_label)\n",
    "\n",
    "# encoding test labels \n",
    "encoder.fit(Y_test_label)\n",
    "Y_test = encoder.transform(Y_test_label)\n",
    "\n",
    "#Total Number of Continous and Categorical features in the training set\n",
    "num_cols = X_train._get_numeric_data().columns\n",
    "print(\"Number of numeric features:\",num_cols.size)\n",
    "#list(set(X_train.columns) - set(num_cols))\n",
    "\n",
    "\n",
    "names_of_predictors = list(X_train.columns.values)\n",
    "\n",
    "# Scaling the Train and Test feature set \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9936101573619309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled, Y_train)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Epochs & Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model1():\n",
    "    #Create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(48, input_dim = 200,\n",
    "                         kernel_initializer='uniform', activation='relu', ))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(24, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(12, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(6, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.982862 using {'batch_size': 10, 'epochs': 100}\n",
      "0.975109 (0.000575) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.977829 (0.002164) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.982862 (0.002665) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.974565 (0.006844) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.979597 (0.003176) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.981910 (0.003002) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.976333 (0.002332) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.979461 (0.002710) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.980277 (0.003416) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.975925 (0.002600) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.977557 (0.004171) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.982046 (0.006327) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.974429 (0.004293) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.979053 (0.001499) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.978509 (0.001711) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.970076 (0.008023) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.975517 (0.003665) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.980414 (0.002642) with: {'batch_size': 100, 'epochs': 100}\n",
      "Running time: 201.25165399999997 Seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=Create_model1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.982454 using {'batch_size': 10, 'epochs': 100}\n",
      "0.966948 (0.008833) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.980005 (0.003181) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.982454 (0.004934) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.974157 (0.004117) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.981910 (0.004034) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.982182 (0.004428) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.972252 (0.006229) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.980822 (0.004668) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.981910 (0.004502) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.965316 (0.008079) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.978645 (0.003673) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.982182 (0.004949) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.958379 (0.004514) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.980686 (0.004116) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.982182 (0.004859) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.941513 (0.006524) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.978917 (0.004076) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.976877 (0.001572) with: {'batch_size': 100, 'epochs': 100}\n",
      "Running time: 181.738535 Seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=Create_model1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
